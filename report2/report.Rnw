\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}

\title{Neural Networks Assignment II \\
    Learning a rule}
\author{Xeryus Stokkel}

\begin{document}

\maketitle

\section{Introduction}
A perceptron can be used to store a data set if the network is sufficiently large enough. If new unseen data is presented to the perceptron then it can classify the new data points based on the points it has seen before. The classification happens based on the separating hyperplane that the perceptron has found, this hyperplane separates all instances of the positive class from the instances of the negative class.

One of the properties of the separating hyperplane is its optimality, that is, its distance to the closest data point(s). When this distance is the highest possible then the hyperplane is exactly in between the two classes of the dataset. In this case the probability of misclassifying new data is the lowest of any of the possible separating hyperplanes.

\section{Method}

\section{Results and discussion}

<<minover, cache=F, echo=F, dev='tikz', fig.pos='t', fig.height=5, fig.cap='Generalisation error of networks trained minover network'>>=
dat = read.csv('results.csv', header=T)
e = aggregate(cbind(err, t) ~ alpha, FUN=mean, data=dat)

plot(e$alpha, e$err,
     type="l",
     xlab="$\\alpha$",
     ylab="$\\epsilon_g$",
     ylim=c(0, 0.55))

@


\end{document}
