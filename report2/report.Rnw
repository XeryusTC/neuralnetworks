\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\usepackage{listings}
\usepackage[hidelinks]{hyperref}
\usepackage[all]{hypcap}

\lstset{
    basicstyle=\footnotesize,
	breakatwhitespace=false,
	breaklines=true,
	frame=single,
	keepspaces=true,
	numbers=left,
	numbersep=5pt,
	numberstyle=\tiny,
	tabsize=4,
	rulecolor=\color{black},
	showspaces=false,
	showstringspaces=false,
	showtabs=false,
	title=\lstname,
    language=python,
}

\title{Neural Networks Assignment II \\
    Learning a rule}
\author{Xeryus Stokkel}

\begin{document}

\maketitle

\section{Introduction}
A perceptron can be used to store a data set if the network is sufficiently large enough. If new unseen data is presented to the perceptron then it can classify the new data points based on the points it has seen before. The classification happens based on the separating hyperplane that the perceptron has found, this hyperplane separates all instances of the positive class from the instances of the negative class.

One of the properties of the perceptron is its optimality, that is, the distance of the separating hyperplane to the closest data point(s). When this distance is the highest possible then the hyperplane is exactly in between the two classes of the dataset. In this case the probability of misclassifying new data is the lowest of any of the possible separating hyperplanes. When noise is added to the dataset the perceptron of optimal stability is the least likely to misclassifying the data points out of all the possible perceptrons.

\section{Method}
The dataset $\mathbb{D} = \{ \bm{\xi}^\mu, S_R^\mu = S_R(\bm{\xi}^\mu)\}_{\mu=1}^P$ is obtained by generating $P$ vectors $\bm{\xi}^\mu$ with independent random Gaussian components of mean zero and variance one. A teacher perceptron provides the labels; $S_R(\bm{\xi}^\mu) = \text{sign}(\mathbf{w}^* \cdot \bm{\xi}^\mu)$. No noise is added to the labels.

It is the goal of the student perceptron to find $\mathbf{w}$ such that it approximates $\mathbf{w}^*$. To find the most optimal hyperplane the minover training algorithm is used. This algorithm aims to find a vector $\mathbf{w}$ such that $\kappa(\mathbf{w})$ is maximised where
\[ \kappa(\mathbf{w}) = \min_\mu = \frac{\mathbf{w} \cdot \bm{\xi}^\mu S_R^\mu}{|\mathbf{w}|} \]
resulting in a $\mathbf{w}$ that has a small angle to $\mathbf{w}^*$. This angle can also be measured as the generalization error
\[ \varepsilon_g = \frac{1}{\pi} \arccos \left( \frac{\mathbf{w} \cdot \mathbf{w}^*}{|\mathbf{w}||\mathbf{w}^*|} \right) \]
which goes towards 0 as $\mathbf{w}$ becomes parallel to $\mathbf{w}^*$. The implementation of the minover algorithm can be found in \autoref{code:minover}. Note that the implementation doesn't divide by $|\mathbf{w}|$ in the calculation of $\kappa(\mathbf{w})$. After the update step $\mathbf{w}$ is normalized to be of unit length so that it becomes easier to determine when to stop. Training is stopped when $\kappa(\mathbf{w})$ does not change significantly over $P$ rounds of training.

\lstinputlisting[caption={Implementation of the minover training algorithm}, label={code:minover}, firstline=29, lastline=39]{../minover.py}

To determine how well the minover algorithm is able to learn the teacher's rule, its generalisation error as a function of the relative size of the data set is tested. The number of input neurons to the network is set to $N = 20$, the size of the dataset is $P = \alpha N$. $\alpha$ is varied to find the generalisation error as the size of the data set changes. Values of $\alpha$ were picked as follows so that $P = 1, 3, 5, \ldots 181$. At most $t_\text{max} = 25000$ update steps are allowed to keep computation time within a reasonable limit. This experiment was repeated $n_D = 20$ times to get a reasonable average generalisation error for each $\alpha$ without needing too much training time.

As a comparison a separate perceptron is trained with the Rosenblat algorithm and its generalisation error is calculated. Most parameters for this perceptron are the same, $t_\text{max}$ has been decreased to $1000$ since the amount of training steps for training a perceptron also depends on $P$. Training of this perceptron was stopped when all $P$ examples were classified correctly or $t_{\max} \times P$ training steps were reached.

\section{Results and discussion}

<<minover, cache=F, echo=F, dev='tikz', fig.pos='t', fig.height=5, fig.cap='Average generalisation error $\\varepsilon_g$ as a function of $\\alpha = \\frac{P}{N}$. The black line is the observed generalisation error with its standard deviation in grey. The blue line is the theoretical probability of misclassification for $N = 20$. The red line is the generalisation error $\\varepsilon_g$ for the Rosenblat perceptron.'>>=
dat = read.csv('results.csv', header=T)
e = aggregate(cbind(err, t) ~ alpha, FUN=mean, data=dat)
e_std = aggregate(cbind(err, t) ~ alpha, FUN=sd, data=dat)

epsilon = read.csv('epsilon.csv', header=T)

dat = read.csv('rosenblat_e_g_results.csv', header=T)
e.rosenblat = aggregate(cbind(err, t) ~ alpha, FUN=mean, data=dat)

CI.up = as.numeric(e$err) + as.numeric(e_std$err)
CI.dn = as.numeric(e$err) - as.numeric(e_std$err)

plot(e$alpha, e$err,
     xlab="$\\alpha$",
     ylab="$\\varepsilon_g$",
     ylim=c(0, 0.55),
     type='n',
     bty='l',
     axes=F)
axis(1, pos=0, at=0:ceiling(max(e$alpha)))
axis(2, pos=0)
polygon(c(e_std$alpha, rev(e_std$alpha)), c(CI.dn, rev(CI.up)),
        col='grey80', border=NA)
lines(epsilon$alpha[epsilon$alpha<=max(e$alpha)],
      epsilon$epsilon[epsilon$alpha<=max(e$alpha)],
      col='blue')
lines(e.rosenblat$alpha, e.rosenblat$err,
      col='red')
lines(e$alpha, e$err, type='l', col='black')
@

The resulting generalisation error $\varepsilon_g$ as a function of $\alpha$ can be found in \autoref{fig:minover}. It shows the observed generalisation error as a black line with its standard deviation in grey. A blue line that shows the theoretical probability of misclassification $\epsilon(P, N)$ is shown in blue, this is calculated according to
\[ \epsilon(P, N) = \frac{C(P, N-1)}{2C(P,N)} \]
where $C(P, N)$ is the recursive function
\[C(1,N) = C(P,1) = 2 \]
\[C(P, N) = C(P-1,N) + C(P-1, N-1) \]

The observed generalisation error in \autoref{fig:minover} shows a clear asymptotic descend. $\varepsilon_g$ begins a near linear descend around $\alpha=4.5$ but the standard deviation keeps slowly decreasing. This means that although the perceptrons are not able to learn much better from the increasing size of the data set, the variation between the different perceptrons decreases, so they are more likely to learn the same rule.

What is most interesting about the graph is that $\varepsilon_g$ and $\epsilon(P,N)$ are very distinct. While $\varepsilon_g$ shows immediate asymptotic behaviour, $\epsilon(P,N)$ is constant while $\alpha \leq 1$ and only only sets into its asymptotic behaviour after that. This means that the network is learning the function before it has used its full storage capacity. This is not surprising since the minover algorithm is aimed at learning a discriminative function, its goal is not to store the the labels of individual data points.

\autoref{fig:minover} shows that $\varepsilon_g$ and $\epsilon(P,N)$ cross each other at $\alpha \approx 5$. This indicates that the perceptron is not fully able to match $\mathbf{w}$ to $\mathbf{w}^*$ as expected. It also shows that $\varepsilon_g$ is already in a flatter region so increasing the size of the data set will not increase the performance of the perceptron as much as the theory predicts.

When we compare the perceptron trained using the Rosenblat algorithm to the perceptron trained using the minover algorithm we notice a few changes. Their performance for low $\alpha$ is comparable, however after $\alpha \approx 4.5$ the Rosenblat perceptron keeps following an asymptotic decrease where the minover perceptron follows a more linear decrease. The Rosenblat perceptron performs better than predicted by the theory, it matches the predicted $\epsilon(P,N)$ only from $\alpha = 8$ onwards. This indicates that the Rosenblat algorithm is better able to generalize from noise free data than the minover algorithm.

\end{document}
